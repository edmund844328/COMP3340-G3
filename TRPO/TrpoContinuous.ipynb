{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2eb8b14c-63ba-4178-8b6b-568c3e400426",
   "metadata": {},
   "source": [
    "### Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e62342f-57fb-462b-b867-299e2472a2c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install torch\n",
    "!pip install sb3-contrib\n",
    "!pip install stable_baselines\n",
    "!pip install gym[box2d]==0.21.0\n",
    "!pip install tqdm\n",
    "conda install numba cudatoolkit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae02ba74-1b06-4979-84ef-0e841604c3ff",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7bdb3d25-d7e8-4cf5-8a78-3d63610b2ba0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gym version:  0.21.0\n",
      "torch version:  1.13.0\n",
      "Device set to : cpu\n"
     ]
    }
   ],
   "source": [
    "import torch as th\n",
    "import gym\n",
    "\n",
    "from sb3_contrib import TRPO\n",
    "from tqdm import tqdm, trange\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Gym version: \",gym.__version__)\n",
    "print(\"torch version: \",th.__version__)\n",
    "if(th.cuda.is_available()): \n",
    "    device = torch.device('cuda:0') \n",
    "    th.cuda.empty_cache()\n",
    "    print(\"Device set to : \" + str(torch.cuda.get_device_name(device)))\n",
    "else:\n",
    "    print(\"Device set to : cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48385799-16a9-41a6-87c9-fe93e9cfed97",
   "metadata": {},
   "source": [
    "### Test The Gym Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b79fc3c-cf53-4ff5-b891-95d73d0d02ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample action:  [-0.28576493 -0.3793123 ]\n",
      "Obsevation space action:  (8,)\n",
      "sample observation:  [-1.6546565   0.7256259   2.1247556  -1.0082954  -0.98624927 -1.4867046\n",
      " -1.0273969  -0.9586193 ]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('LunarLanderContinuous-v2')\n",
    "env.reset()\n",
    "print(\"sample action: \", env.action_space.sample())\n",
    "print(\"Obsevation space action: \", env.observation_space.shape)\n",
    "print(\"sample observation: \", env.observation_space.sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18dcee56-eed4-40f2-8583-aa8c38f355df",
   "metadata": {},
   "source": [
    "### Train And Test the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fa1212-6079-4083-9b3b-918634533daa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████████████████████████▌                                         | 5/10 [00:13<00:13,  2.69s/it]"
     ]
    }
   ],
   "source": [
    "num_features = 128\n",
    "# Custom actor (pi) and value function (vf) networks\n",
    "policy_kwargs = dict(activation_fn=th.nn.ReLU,\n",
    "                     net_arch=[dict(pi=[num_features, num_features, num_features], vf=[num_features, num_features, num_features])])     \n",
    "\n",
    "env = gym.make('LunarLanderContinuous-v2')\n",
    "model = TRPO('MlpPolicy', env, policy_kwargs=policy_kwargs, gamma=0.99, target_kl=0.01, batch_size=64,verbose=1)\n",
    "\n",
    "episodes = 10\n",
    "\n",
    "reward_per_episode = []\n",
    "\n",
    "\n",
    "for episode in trange(episodes):\n",
    "    model = model.learn(total_timesteps=2048, log_interval=4)\n",
    "    state = env.reset()\n",
    "    total_reward, total_step = 0, 0\n",
    "    while True:\n",
    "        action, _state = model.predict(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        total_step += 1\n",
    "        if done:\n",
    "            env.reset()\n",
    "            reward_per_episode.append(total_reward)\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fed18c-d7b5-4ca1-a734-ff5d96b735fc",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1449a0c0-261b-402f-9e26-f892ec96fd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(reward_per_episode)\n",
    "plt.title(\"Rewards per episode\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
